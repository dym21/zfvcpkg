From a1e69b26da7904c50a6eb2264dadc4bc2f1de684 Mon Sep 17 00:00:00 2001
From: Your Name <you@example.com>
Date: Sun, 13 Jul 2025 10:10:26 +0800
Subject: [PATCH] fixerror

---
 cmake/CMakeLists.txt                          |  2 +-
 cmake/external/abseil-cpp.cmake               |  2 +-
 cmake/external/cudnn_frontend.cmake           |  8 ++++++
 cmake/onnxruntime.cmake                       |  2 +-
 cmake/onnxruntime_providers_cuda.cmake        |  5 ++--
 cmake/onnxruntime_providers_tensorrt.cmake    | 22 ++++++++++------
 cmake/tensorboard/compat/proto/CMakeLists.txt | 25 ++++++++++++-------
 .../core/providers/cpu/controlflow/loop.cc    |  4 +--
 8 files changed, 47 insertions(+), 23 deletions(-)

diff --git a/cmake/CMakeLists.txt b/cmake/CMakeLists.txt
index 5e68990..61cb3eb 100644
--- a/cmake/CMakeLists.txt
+++ b/cmake/CMakeLists.txt
@@ -1166,7 +1166,7 @@ function(onnxruntime_set_compile_flags target_name)
       target_compile_definitions(${target_name} PRIVATE ENABLE_ATEN)
     endif()
 
-    set_target_properties(${target_name} PROPERTIES COMPILE_WARNING_AS_ERROR ON)
+    set_target_properties(${target_name} PROPERTIES COMPILE_WARNING_AS_ERROR OFF)
     if (onnxruntime_USE_CUDA)
       # Suppress a "conversion_function_not_usable" warning in gsl/span
       target_compile_options(${target_name} PRIVATE "$<$<COMPILE_LANGUAGE:CUDA>:SHELL:-Xcudafe \"--diag_suppress=conversion_function_not_usable\">")
diff --git a/cmake/external/abseil-cpp.cmake b/cmake/external/abseil-cpp.cmake
index cd93a8d..0e48bcd 100644
--- a/cmake/external/abseil-cpp.cmake
+++ b/cmake/external/abseil-cpp.cmake
@@ -28,7 +28,7 @@ onnxruntime_fetchcontent_declare(
     URL_HASH SHA1=${DEP_SHA1_abseil_cpp}
     EXCLUDE_FROM_ALL
     PATCH_COMMAND ${ABSL_PATCH_COMMAND}
-    FIND_PACKAGE_ARGS 20240722 NAMES absl
+    FIND_PACKAGE_ARGS NAMES absl CONFIG
 )
 
 onnxruntime_fetchcontent_makeavailable(abseil_cpp)
diff --git a/cmake/external/cudnn_frontend.cmake b/cmake/external/cudnn_frontend.cmake
index 8642607..65e4345 100644
--- a/cmake/external/cudnn_frontend.cmake
+++ b/cmake/external/cudnn_frontend.cmake
@@ -1,4 +1,12 @@
 
+find_path(CUDNN_FRONTEND_INCLUDE_DIRS NAMES "NvInferVersion.h" "cudnn_frontend.h")
+if(CUDNN_FRONTEND_INCLUDE_DIRS)
+  add_library(cudnn_frontend INTERFACE)
+  target_include_directories(cudnn_frontend INTERFACE ${CUDNN_FRONTEND_INCLUDE_DIRS})
+  return()
+endif()
+
+
 onnxruntime_fetchcontent_declare(
   cudnn_frontend
   URL ${DEP_URL_cudnn_frontend}
diff --git a/cmake/onnxruntime.cmake b/cmake/onnxruntime.cmake
index 6c1d448..19e36c0 100644
--- a/cmake/onnxruntime.cmake
+++ b/cmake/onnxruntime.cmake
@@ -289,7 +289,7 @@ install(TARGETS onnxruntime
         ARCHIVE   DESTINATION ${CMAKE_INSTALL_LIBDIR}
         LIBRARY   DESTINATION ${CMAKE_INSTALL_LIBDIR}
         RUNTIME   DESTINATION ${CMAKE_INSTALL_BINDIR}
-        FRAMEWORK DESTINATION ${CMAKE_INSTALL_BINDIR})
+		FRAMEWORK DESTINATION ${CMAKE_INSTALL_LIBDIR}
 
 
 if (WIN32 AND NOT CMAKE_CXX_STANDARD_LIBRARIES MATCHES kernel32.lib)
diff --git a/cmake/onnxruntime_providers_cuda.cmake b/cmake/onnxruntime_providers_cuda.cmake
index 6a7510a..3515fb7 100644
--- a/cmake/onnxruntime_providers_cuda.cmake
+++ b/cmake/onnxruntime_providers_cuda.cmake
@@ -253,8 +253,9 @@
       target_link_libraries(${target} PRIVATE CUDA::cuda_driver)
     endif()
 
-    include(cutlass)
-    target_include_directories(${target} PRIVATE ${cutlass_SOURCE_DIR}/include ${cutlass_SOURCE_DIR}/examples ${cutlass_SOURCE_DIR}/tools/util/include)
+	find_package(NvidiaCutlass REQUIRED)
+	target_link_libraries(${target} PRIVATE nvidia::cutlass::cutlass)
+
     target_link_libraries(${target} PRIVATE Eigen3::Eigen)
     target_include_directories(${target} PRIVATE ${ONNXRUNTIME_ROOT} ${CMAKE_CURRENT_BINARY_DIR} PUBLIC ${CUDAToolkit_INCLUDE_DIRS})
     # ${CMAKE_CURRENT_BINARY_DIR} is so that #include "onnxruntime_config.h" inside tensor_shape.h is found
diff --git a/cmake/onnxruntime_providers_tensorrt.cmake b/cmake/onnxruntime_providers_tensorrt.cmake
index 59c7db9..08e76cf 100644
--- a/cmake/onnxruntime_providers_tensorrt.cmake
+++ b/cmake/onnxruntime_providers_tensorrt.cmake
@@ -10,15 +10,22 @@
   set(BUILD_LIBRARY_ONLY 1)
   add_definitions("-DONNX_ML=1")
   add_definitions("-DONNX_NAMESPACE=onnx")
-  set(CUDA_INCLUDE_DIRS ${CUDAToolkit_INCLUDE_DIRS})
-  set(TENSORRT_ROOT ${onnxruntime_TENSORRT_HOME})
+if(DEFINED onnxruntime_TENSORRT_HOME)
+  set(TENSORRT_ROOT ${onnxruntime_TENSORRT_HOME}) # expect CMake configuration input
+else()
+  find_package(CUDAToolkit REQUIRED)
+  get_filename_component(TENSORRT_ROOT "${CUDAToolkit_LIBRARY_ROOT}" ABSOLUTE)
+  message(STATUS "Guessing TensorRT with CUDAToolkit_LIBRARY_ROOT: ${TENSORRT_ROOT}")
+endif()
+
   set(OLD_CMAKE_CXX_FLAGS ${CMAKE_CXX_FLAGS})
   set(PROTOBUF_LIBRARY ${PROTOBUF_LIB})
   if (WIN32)
     set(OLD_CMAKE_CUDA_FLAGS ${CMAKE_CUDA_FLAGS})
     set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} /wd4099 /wd4551 /wd4505 /wd4515 /wd4706 /wd4456 /wd4324 /wd4701 /wd4804 /wd4702 /wd4458 /wd4703")
-    if (CMAKE_BUILD_TYPE STREQUAL "Debug")
-      set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} /wd4805")
+	if (WIN32 AND CMAKE_BUILD_TYPE STREQUAL "Debug")
+		option(onnxruntime_USE_TENSORRT_BUILTIN_PARSER "Use tensorrt oss parser instead of TRT built-in parser" OFF)
+
     endif()
     set(CMAKE_CUDA_FLAGS "${CMAKE_CUDA_FLAGS} -include algorithm")
     set(DISABLED_WARNINGS_FOR_TRT /wd4456)
@@ -28,9 +35,10 @@
   endif()
   set(CXX_VERSION_DEFINED TRUE)
 
-  find_path(TENSORRT_INCLUDE_DIR NvInfer.h
-    HINTS ${TENSORRT_ROOT}
-    PATH_SUFFIXES include)
+find_path(TENSORRT_INCLUDE_DIR NvInfer.h REQUIRED
+  HINTS ${TENSORRT_ROOT}
+  PATH_SUFFIXES include)
+
 
   file(READ ${TENSORRT_INCLUDE_DIR}/NvInferVersion.h NVINFER_VER_CONTENT)
   string(REGEX MATCH "define NV_TENSORRT_MAJOR * +([0-9]+)" NV_TENSORRT_MAJOR "${NVINFER_VER_CONTENT}")
diff --git a/cmake/tensorboard/compat/proto/CMakeLists.txt b/cmake/tensorboard/compat/proto/CMakeLists.txt
index ad31e40..04ed1ce 100644
--- a/cmake/tensorboard/compat/proto/CMakeLists.txt
+++ b/cmake/tensorboard/compat/proto/CMakeLists.txt
@@ -1,14 +1,21 @@
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 
-FetchContent_Declare(
-  tensorboard
-  URL ${DEP_URL_tensorboard}
-  URL_HASH SHA1=${DEP_SHA1_tensorboard}
-)
-FetchContent_MakeAvailable(tensorboard)
-
-set(TENSORBOARD_ROOT ${tensorboard_SOURCE_DIR})
+if(onnxruntime_USE_VCPKG)
+  if(NOT DEFINED TENSORBOARD_ROOT)
+    message(FATAL_ERROR "TENSORBOARD_ROOT is not defined. Please set the value in the CMake command.")
+  endif()
+  find_path(PROTOBUF_IMPORT_DIR NAMES "google/protobuf/api.proto" REQUIRED)
+else()
+  FetchContent_Declare(
+    tensorboard
+    URL ${DEP_URL_tensorboard}
+    URL_HASH SHA1=${DEP_SHA1_tensorboard}
+  )
+  FetchContent_MakeAvailable(tensorboard)
+  set(TENSORBOARD_ROOT ${tensorboard_SOURCE_DIR})
+  set(PROTOBUF_IMPORT_DIR ${protobuf_SOURCE_DIR}/src)
+endif()
 
 # tensorboard protos
 file(GLOB_RECURSE tensorboard_proto_srcs CONFIGURE_DEPENDS
@@ -16,7 +23,7 @@ file(GLOB_RECURSE tensorboard_proto_srcs CONFIGURE_DEPENDS
 )
 
 add_library(tensorboard STATIC ${tensorboard_proto_srcs})
-onnxruntime_protobuf_generate(APPEND_PATH IMPORT_DIRS ${tensorboard_SOURCE_DIR} ${protobuf_SOURCE_DIR}/src TARGET tensorboard)
+onnxruntime_protobuf_generate(APPEND_PATH IMPORT_DIRS ${TENSORBOARD_ROOT} ${PROTOBUF_IMPORT_DIR} TARGET tensorboard)
 onnxruntime_add_include_to_target(tensorboard ${PROTOBUF_LIB})
 target_include_directories(tensorboard PRIVATE ${PROJECT_BINARY_DIR})
 add_dependencies(tensorboard ${onnxruntime_EXTERNAL_DEPENDENCIES})
diff --git a/onnxruntime/core/providers/cpu/controlflow/loop.cc b/onnxruntime/core/providers/cpu/controlflow/loop.cc
index b33b1f1..6d3f62d 100644
--- a/onnxruntime/core/providers/cpu/controlflow/loop.cc
+++ b/onnxruntime/core/providers/cpu/controlflow/loop.cc
@@ -244,8 +244,8 @@ static Status ConcatenateCpuOutput(void* /*stream*/,
 
   // we can't easily use a C++ template for the tensor element type,
   // so use a span for some protection but work in bytes
-  gsl::span<std::byte> output_span = gsl::make_span<std::byte>(static_cast<std::byte*>(output),
-                                                               output_size_in_bytes);
+gsl::span<std::byte> output_span = gsl::make_span<std::byte>(static_cast<std::byte*>(output), output_size_in_bytes);
+
 
   for (size_t i = 0, num_iterations = per_iteration_output.size(); i < num_iterations; ++i) {
     auto& ort_value = per_iteration_output[i];
-- 
2.25.1

